{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Datasest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,Train,address):\n",
    "        self.address = address\n",
    "        self.Train = Train\n",
    "        self.labels = []\n",
    "        for i in Train:\n",
    "            self.labels.append(text_transform(i[1]))\n",
    "        self.labels = pad_sequence(self.labels , padding_value=PAD_IDX)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img = cv2.imread(self.address + self.Train[index][0])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.rot90(img)\n",
    "        img = cv2.resize(img,(64,img.shape[0]),interpolation = cv2.INTER_AREA)\n",
    "        if img.shape[0] > 1600:\n",
    "            img = cv2.resize(img,(64,1600),interpolation = cv2.INTER_AREA)\n",
    "        if img.shape[0] < 1600:\n",
    "            padded_array = np.zeros((1600,64))\n",
    "            shape = np.shape(img)\n",
    "            padded_array[:shape[0],-shape[1]:] = img\n",
    "            img = padded_array\n",
    "\n",
    "        return torch.from_numpy(img).double(), self.labels[:,index], self.Train[index][0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please give addresses of train and test files containing information of images and their lables. Please see refernece Train and Test .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/tukl/data/HWR/Final_implementation/upti-2/train.txt','r')\n",
    "lines = f.read().split('\\n')\n",
    "Train = [tuple(line.split('\\t')) for line in lines]\n",
    "f.close()\n",
    "\n",
    "f3 = open('/home/tukl/data/HWR/Final_implementation/upti-2/test.txt','r')\n",
    "lines = f3.read().split('\\n')\n",
    "Test = [tuple(line.split('\\t')) for line in lines]\n",
    "f3.close()\n",
    "\n",
    "address = \"/home/tukl/data/HWR/Final_implementation/upti-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing end of file spaces from train/test files data\n",
    "Train.pop(len(Train)-1)\n",
    "Test.pop(len(Test)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Train and Test number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Train))\n",
    "# print(len(Val))\n",
    "print(len(Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary preparation from Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForVocab = []\n",
    "for vcl in Train:\n",
    "    temp = [s for s in vcl[1]]\n",
    "    ForVocab.append(temp)\n",
    "    \n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "vocab_transform = build_vocab_from_iterator(ForVocab,\n",
    "                                            min_freq=1,\n",
    "                                            specials=special_symbols,\n",
    "                                            special_first=True)\n",
    "vocab_transform.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transforms to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def token_transform(line:str):\n",
    "    chars = [char for char in line]\n",
    "    return chars\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = sequential_transforms(token_transform, #Tokenization\n",
    "                                           vocab_transform, #Numericalization\n",
    "                                           tensor_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data sets to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrData = MyDataset(Train,address)\n",
    "# VlData = MyDataset(Val,address)\n",
    "TestData = MyDataset(Test,address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TrData, batch_size=batch_size,\n",
    "     num_workers=num_workers, shuffle = True)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(VlData, batch_size=batch_size,\n",
    "#      num_workers=num_workers, shuffle = True)\n",
    "\n",
    "test_loader= torch.utils.data.DataLoader(TestData, batch_size=batch_size,\n",
    "     num_workers=num_workers, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Token Embedding and Positional Encoding used for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the maskings used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Proposed in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTrans(nn.Module):\n",
    "#     def __init__(self,d_model=256):\n",
    "#         super(ConvTrans, self).__init__()\n",
    "        \n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "#                  src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(ConvTrans, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "#         self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "#         self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 48, 3, padding=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(48, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d((1,2)),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 96, 3, padding=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(96, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d((1,2)),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(128, emb_size, 4),\n",
    "            nn.BatchNorm2d(emb_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,\n",
    "                x: Tensor,\n",
    "                trg: Tensor):\n",
    "#                 src_mask: Tensor,\n",
    "#                 tgt_mask: Tensor,\n",
    "#                 src_padding_mask: Tensor,\n",
    "#                 tgt_padding_mask: Tensor,\n",
    "#                 memory_key_padding_mask: Tensor):\n",
    "        \n",
    "       \n",
    "                                \n",
    "        x = torch.unsqueeze(x,1)\n",
    "        x = self.conv1(x)\n",
    "#         print(\"Conv1 Shape: \",x.shape)\n",
    "        x = self.conv2(x)\n",
    "#         print(\"Conv2 Shape: \",x.shape)\n",
    "        x = self.conv3(x)\n",
    "#         print(\"Conv3 Shape: \",x.shape)\n",
    "        x = self.conv4(x)\n",
    "#         print(\"Conv4 Shape: \",x.shape)\n",
    "        x = self.conv5(x)\n",
    "#         print(\"Conv5 Shape: \",x.shape)\n",
    "        x = self.conv6(x)\n",
    "#         print(\"Conv6 Shape: \",x.shape)\n",
    "        x = self.conv7(x)\n",
    "#         print(\"Conv7 Shape: \",x.shape)\n",
    "\n",
    "        x = x.squeeze(-1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        \n",
    "#         x = x.view(-1,x.shape[0],self.emb_size)\n",
    "        \n",
    "#         print(x.shape)\n",
    "#         print(\"TRG shape before transpose\",trg.shape)\n",
    "        trg = torch.transpose(trg,0,1)\n",
    "#         print(\"TRG shape after transpose\",trg.shape)\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(x, trg)\n",
    "        src_padding_mask = src_padding_mask[:,:][0]\n",
    "#         print (src_mask.shape, tgt_mask.shape,src_padding_mask.shape,tgt_padding_mask.shape)\n",
    "        src_emb = self.positional_encoding(x)\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "#         tgt_emb = tgt_emb.view(-1,tgt_emb.shape[0],self.emb_size)\n",
    "#         print(\"Feature Maps Dimension before Transformer:\",x.shape)\n",
    "#         print(\"TGT_EMB Dimension before Transformer:\",tgt_emb.shape)\n",
    "        \n",
    "        \n",
    "#         outs = self.transformer(src_emb, tgt_emb)\n",
    "        outs = self.transformer(src_emb, tgt_emb, None, tgt_mask, None, \n",
    "                                None, tgt_padding_mask)\n",
    "#         outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "#                                 None, tgt_padding_mask)\n",
    "#                                 src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        return self.generator(outs)                        \n",
    "        \n",
    "    def encode(self, src: Tensor):\n",
    "        \n",
    "        x = src\n",
    "        x = self.conv1(x)\n",
    "#         print(\"Conv1 Shape: \",x.shape)\n",
    "        x = self.conv2(x)\n",
    "#         print(\"Conv2 Shape: \",x.shape)\n",
    "        x = self.conv3(x)\n",
    "#         print(\"Conv3 Shape: \",x.shape)\n",
    "        x = self.conv4(x)\n",
    "#         print(\"Conv4 Shape: \",x.shape)\n",
    "        x = self.conv5(x)\n",
    "#         print(\"Conv5 Shape: \",x.shape)\n",
    "        x = self.conv6(x)\n",
    "#         print(\"Conv6 Shape: \",x.shape)\n",
    "        x = self.conv7(x)\n",
    "#         print(\"Conv7 Shape: \",x.shape)\n",
    "        x = x.squeeze(-1)\n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        \n",
    "#         return self.transformer.encoder(self.positional_encoding(src))\n",
    "        return self.transformer.encoder(self.positional_encoding(x))\n",
    "\n",
    "#     def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "#         return self.transformer.decoder(self.positional_encoding(\n",
    "#                           self.tgt_tok_emb(tgt)), memory, tgt_mask)\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        \n",
    "        tgt = torch.transpose(tgt,0,1)\n",
    "#         print(\"TGT:\",tgt.shape)\n",
    "#         print(\"Memory:\",memory.shape)\n",
    "#         print(\"Tgt_Mask:\",tgt_mask.shape)\n",
    "        \n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "    \n",
    "    def sampleFunc(self,x):\n",
    "        \n",
    "        x = torch.unsqueeze(x,0)\n",
    "        x = torch.unsqueeze(x,0)\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.view(-1,x.shape[0],self.emb_size)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# model = ConvTrans()\n",
    "# model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Hyperparameters, Optimizer Function, Loss Function etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Â¶torch.manual_seed(0)\n",
    "\n",
    "TGT_VOCAB_SIZE = len(vocab_transform)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = ConvTrans(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
    "                             NHEAD, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "transformer = transformer.double()\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0003, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading of a model to use pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load('Epoch-2 checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer,train_loader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "#     train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    for src, tgt,add in train_loader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        tgt_out = torch.transpose(tgt_out,0,1)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    \n",
    "\n",
    "    return losses / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model,val_loader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    Accuracy = 0\n",
    "\n",
    "#     val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt,add in val_loader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "#         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, torch.transpose(tgt_input,0,1))\n",
    "        \n",
    "        Out,Ind = torch.max(logits,2)\n",
    "        \n",
    "#         tgt_out = tgt[1:, :]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        tgt_out = torch.transpose(tgt_out,0,1)\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, axis=-1)\n",
    "        mask = torch.logical_not(tgt_padding_mask).transpose(0,1)\n",
    "        correct += ((preds == tgt_out)*mask).sum()\n",
    "        total += mask.sum()\n",
    "        \n",
    "    Accuracy = (correct/total)*100\n",
    "        \n",
    " \n",
    "    return losses / len(val_loader) , Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,train_loader)\n",
    "    end_time = timer()\n",
    "#     gc.collect()\n",
    "#     time.sleep(3)\n",
    "#     torch.cuda.empty_cache()\n",
    "    val_loss, Accuracy = evaluate(transformer,test_loader)\n",
    "    torch.save(transformer.state_dict(), 'Epoch-{} checkpoint.pth'.format(epoch))\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"f\" Accuracy = {Accuracy:.3f}%\"))\n",
    "    fw = open(\"Performance.txt\",'a')\n",
    "    fw.write((f\"\\nEpoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"f\" Accuracy = {Accuracy:.3f}%\"))\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for using Beam Search for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_beam(images, model, max_length=150, device=DEVICE, flip=False,k=5):\n",
    "    with torch.no_grad():\n",
    "        inp = images\n",
    "        \n",
    "        predicted_indices = torch.empty((k,inp.shape[0],max_length)).type(torch.LongTensor).to(device)\n",
    "        predicted_probs = torch.empty((k,inp.shape[0],max_length)).type(torch.FloatTensor).to(device)\n",
    "        prediction_scores = torch.zeros((k,inp.shape[0])).type(torch.FloatTensor).to(device)\n",
    "\n",
    "        features = model.encode(inp)\n",
    "        \n",
    "        indices = torch.LongTensor([BOS_IDX] + [PAD_IDX] * (max_length - 1)).repeat(inp.shape[0],1).to(device) # (B, l_max)\n",
    "#         print(indices.shape)\n",
    "        tgt_mask = (generate_square_subsequent_mask(indices.size(1)).type(torch.bool)).to(device)\n",
    "        \n",
    "#         print(indices.shape)\n",
    "#         print(features.shape)\n",
    "#         print(tgt_mask.shape)\n",
    "        scores = model.decode(indices, features, tgt_mask) # (B, l_max, V)\n",
    "        \n",
    "        scores = torch.transpose(scores,0,1)\n",
    "        \n",
    "        scores = model.generator(scores)\n",
    "        parent_node_indices, parent_node_probs = top_k(scores,k) # (k, B, l_max), (k, B, l_max) \n",
    "        \n",
    "        indices = torch.empty((k, inp.shape[0], max_length)).type(torch.LongTensor).to(device)\n",
    "        indices_probs = torch.zeros(k, inp.shape[0], max_length).type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        indices[:,:,:] = PAD_IDX #[PAD_TOKEN]\n",
    "        indices[:,:,0] = BOS_IDX #[SOS_TOKEN]\n",
    "        indices[:,:,1] = parent_node_indices[:,:,0]\n",
    "        indices_probs[:,:,0] = 1\n",
    "        indices_probs[:,:,1] = parent_node_probs[:,:,0]\n",
    "        \n",
    "        cache_indices = torch.empty((k, k, inp.shape[0], max_length)).type(torch.LongTensor).to(device)\n",
    "        cache_probs = torch.empty((k, k, inp.shape[0], max_length)).type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        for i in range(2, max_length):\n",
    "            \n",
    "            tgt_mask = (generate_square_subsequent_mask(indices.size(-1)).type(torch.bool)).to(device)\n",
    "#             print(indices.shape)\n",
    "#             print(tgt_mask.shape)\n",
    "            \n",
    "            for j in range(k):\n",
    "                temp = model.decode(indices[j,:,:].reshape(inp.shape[0],max_length), features, tgt_mask)\n",
    "                \n",
    "                temp = torch.transpose(temp,0,1)\n",
    "                \n",
    "                temp = model.generator(temp)\n",
    "                cache_indices[j], cache_probs[j] = top_k(temp,k)\n",
    "            \n",
    "            token_probs = indices_probs[:,:,i-1] # (k,B)\n",
    "            nx_token_probs = cache_probs[:,:,:,i-1] #(k,k,B)\n",
    "            \n",
    "            probs = torch.empty((k,k,inp.shape[0]))\n",
    "            for j in range(k):\n",
    "                for l in range(k):\n",
    "                    probs[j,l,:] = token_probs[j,:] + nx_token_probs[j,l,:]\n",
    "\n",
    "            probs = probs.reshape(k*k, inp.shape[0]) # (k^2,B)\n",
    "\n",
    "            best_preds = torch.sort(probs,axis=0)\n",
    "            current_probs = best_preds.values[-k:,:]\n",
    "            index_locs = best_preds.indices[-k:,:]\n",
    "            \n",
    "            p_nodes = index_locs // k\n",
    "            d_nodes = index_locs % k\n",
    "            \n",
    "            indices_temp = indices.detach().clone()\n",
    "            indices_probs_temp = indices_probs.detach().clone()\n",
    "\n",
    "            for y in range(inp.shape[0]):\n",
    "                for j in range(k):\n",
    "                    \n",
    "                    if cache_indices[p_nodes[j,y],d_nodes[j,y],y,i-1] == EOS_IDX and indices[p_nodes[j,y],y,:i].any() != EOS_IDX:\n",
    "                        if (current_probs[j,y].item()/i**0.7 > prediction_scores[j,y].item()) or prediction_scores[j,y]==0:\n",
    "                            predicted_indices[j,y,:] = indices[p_nodes[j,y],y,:]\n",
    "                            predicted_probs[j,y,:] = indices_probs[p_nodes[j,y],y,:]\n",
    "                            predicted_probs[j,y,i] = current_probs[j,y]\n",
    "                            prediction_scores[j,y] = (1/(i**0.7))*current_probs[j,y]\n",
    "                        \n",
    "                    indices_temp[j,y,:] = indices[p_nodes[j,y],y,:]   \n",
    "                    indices_probs_temp[j,y,:] = indices_probs[p_nodes[j,y],y,:]\n",
    "\n",
    "                    indices_temp[j,y,i] = cache_indices[p_nodes[j,y],d_nodes[j,y],y,i-1]\n",
    "                    indices_probs_temp[j,y,i] = current_probs[j,y]\n",
    "                    \n",
    "                indices[:,y,:] = indices_temp[:,y,:]\n",
    "                indices_probs[:,y,:] = indices_probs_temp[:,y,:]\n",
    "\n",
    "    probabilities = prediction_scores\n",
    "    pick = torch.sort(probabilities, axis=0)\n",
    "\n",
    "    for y in range(inp.shape[0]):\n",
    "        indices[0,y,:] = indices[pick.indices[-1,y],y,:]\n",
    "\n",
    "    indices = indices[0,:,:]\n",
    "    indices = indices.cpu().numpy()\n",
    "    \n",
    "    lines = []\n",
    "    for idx in indices:\n",
    "        line=\"\"\n",
    "        pred = \"\".join(vocab_transform.lookup_tokens(list(idx))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"*\")\n",
    "        for char in pred:\n",
    "            if char == '*':\n",
    "                break\n",
    "            else:\n",
    "                line +=char\n",
    "            \n",
    "#         print(line)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def top_k(scores,k):\n",
    "    scores = torch.softmax(scores,axis=-1) #(B, l_max, V)\n",
    "    scores = torch.log(scores)\n",
    "    sorted_score = torch.sort(scores,axis=-1)\n",
    "    sorted_score_indices = sorted_score.indices\n",
    "    sorted_score_probs = sorted_score.values\n",
    "    top_k_indices = (sorted_score_indices[:,:,-k:]) # (B,l_max,k)\n",
    "    top_k_probs = (sorted_score_probs[:,:,-k:])\n",
    "    top_k_indices = torch.swapaxes(torch.swapaxes(top_k_indices,1,2),0,1) #(k, B, l_max)\n",
    "    top_k_probs = torch.swapaxes(torch.swapaxes(top_k_probs,1,2),0,1)\n",
    "    return top_k_indices, top_k_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Evaluating using Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in tqdm(test_loader):\n",
    "    x = torch.unsqueeze(x,1)\n",
    "#     y = torch.transpose(y,0,1)\n",
    "    transformer.to(DEVICE)\n",
    "    transformer.eval()\n",
    "    x = x.to(DEVICE)\n",
    "    lines_x = process_beam(x, transformer, device=DEVICE)\n",
    "    line_y = []\n",
    "    for line in y:\n",
    "        temp = \"\".join(vocab_transform.lookup_tokens(list(line))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "        line_y.append(temp)\n",
    "    for pred, label in zip(lines_x, line_y):\n",
    "            new_row = {'preds':pred, 'labels':label}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    df.to_csv('preds_final.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for using Greedy Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "#     src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory1 = model.encode(src)\n",
    "    \n",
    "    y = []\n",
    "    for w in range(memory1.shape[1]):\n",
    "        memory = memory1[:,w,:]\n",
    "        memory.to(DEVICE)\n",
    "        memory = torch.unsqueeze(memory,1)\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "        yl = []\n",
    "        for i in range(max_len-1):\n",
    "            for i in range (100):\n",
    "                tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
    "                                    .type(torch.bool)).to(DEVICE)\n",
    "                out = transformer.decode(ys,memory,tgt_mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                prob = transformer.generator(out[:, -1])\n",
    "                tpk = torch.topk(prob,k=7,dim=1)\n",
    "                lst = [int(tpk[1][0][v]) for v in range(len(tpk[1][0]))]\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                next_word = next_word.item()\n",
    "                ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "                yl.append(next_word)\n",
    "                if next_word == EOS_IDX:\n",
    "                        break\n",
    "            output = \"\".join(vocab_transform.lookup_tokens(yl)).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "        y.append(output)\n",
    "    return y\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating using Greedy Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "for x1, y,add in tqdm(test_loader):\n",
    "    x1 = torch.unsqueeze(x1,1)\n",
    "    transformer.to(DEVICE)\n",
    "    transformer.eval()\n",
    "    x1 = x1.to(DEVICE)\n",
    "    lines_x = greedy_decode(transformer,x1,src_mask = None,max_len=150,start_symbol=BOS_IDX)\n",
    "        \n",
    "    line_y = []\n",
    "    for line in y:\n",
    "        temp = \"\".join(vocab_transform.lookup_tokens(list(line))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "        line_y.append(temp)\n",
    "    for pred, label, address in zip(lines_x, line_y,add):\n",
    "            new_row = {'Address': address,'preds':pred, 'labels':label}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    df.to_csv('prac_preds_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
